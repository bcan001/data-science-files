classify whether a new mushroom will be edible, poisonous, or other, using classification (supervised learning)

1. calculate entropy of the entire dataset
	- entropy graphs are useful for visualizing information gain from different partitions of a dataset
	- on pg. 60, the bars for each attribute indicate how ACCURATE the attributes are in classification
2. calculate entropy for attribute values (pg. 60). 
	-	Odor is the attribute with the most information gain because it has instances with the lowest entropy
	- this says that odors are characteristic of posinous or edible mushrooms
	- if you were going to build out a model using only 1 characteristic, use odor
3. start building a classification / decision tree based on the best attribute.
4. repeat steps 2 and 3 for each node, calculating the attribute with the most information gain and using that attribute to form the leaf nodes of the tree





* important equations:
entropy = - p1*log(p1) - p2*log(p2)...
p1 = probability that p1 occurs in the data

InformationGain(parent,children) = entropy(parent) - [p(c1)*entropy(c1) + p(c2)*entropy(c2)...]
entropy(c1) = entropy of child c1
p(c1) = probability that c1 occurs in the dataset


The lower the entropy for each of the nodes calculated, the better the attribute is
The higher the information gain of the attribute being tested, the better the attribute will be to use in the model


Laplace correction (for probability of each node):
p(c) = n+1 / n+m+2 
n = # of examples belonging to class c
m = # of examples not belonging to class c





==========
induction tree sequence:
- find attribute with most information gain
- segment the entire dataset based on this attribute
- test the segments for the attribute with the most information gain
- repeat this process until tree is complete









